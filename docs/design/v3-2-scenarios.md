# 自进化智能体系统 — 场景文档

> **版本**: 配合 V3.2 完整设计方案（修订版）
> **日期**: 2026-02-23
> **定位**: 通过具体场景描述系统在完整设计实现后的实际体验，覆盖用户视角和系统视角

---

## 阅读说明

每个场景包含：
- **背景**：发生在什么阶段、什么条件下
- **故事**：完整的过程描述，用户看到什么、系统在做什么
- **结果**：这个场景的收获是什么

场景按照系统生命周期排列，从第一天到数月后。

---

## 一、初生期场景（第 1-3 天）

### 场景 1：Bootstrap 引导——系统的诞生

**背景**：系统刚部署完成，加载了种子规则和 Best Practice 默认配置。Michael 第一次通过 Telegram 发消息。

**用户视角**：

Michael 打开 Telegram，给新 Bot 发了第一条消息：「你好」。

系统回复：

> 你好，Michael！我是你的 AI 助手。在正式开始之前，我想快速了解你一些基本信息，这样能更好地帮你。你也可以随时跳过，后续我会逐步学习。
>
> **方式 A**：如果你有现成的个人介绍、简历或工作描述，直接发给我，我来提取关键信息。
>
> **方式 B**：回答几个简单的问题：
> 1. 你主要做什么工作？最常处理什么类型的任务？
> 2. 你对 AI 助手的使用经验如何？有什么好用/不好用的经验？
> 3. 你偏好什么样的回复风格？（简短直接 / 详细解释 / 看情况）

Michael 回复：

> 我是创业者，做一个 AI 产品。平时要处理产品设计、技术方案、投资人沟通、团队管理。AI 用得多，最烦的是每次要重新解释上下文。回复简短点就行。

系统回复：

> 收到。我整理一下：
> - **角色**：AI 产品创业者
> - **核心任务**：产品设计、技术方案、投资人沟通、团队管理
> - **痛点**：重复解释上下文（这正是我的记忆系统要解决的问题）
> - **风格**：简短直接
>
> 还有几个系统配置想和你确认——这些是我的默认值，你看看要不要调：
>
> 1. **自我分析频率**：每天 1 次（凌晨自动运行）→ 每天/每两天/每周？
> 2. **通知风格**：重要改进才通知 → 还是你想要每日简报？
> 3. **自主程度**：均衡模式（小改动自己做，大改动问你）→ 还是更保守/更大胆？
>
> 直接说数字就行，或者「默认」跳过。

Michael：默认就行。

> 好的，所有配置使用默认值。我们可以开始了。
>
> 以后你随时可以说「调整配置」来修改这些设置。

**系统视角（用户看不到的）**：

```
Bootstrap 引导流程执行：

阶段 1 — 用户背景导入：
  用户选择了方式 B（问答建模）
  提取结果 → 写入 workspace/memory/user/profile.md：
    角色: AI 产品创业者
    核心领域: 产品设计, 技术方案, 投资人沟通, 团队管理
    AI 使用经验: 丰富
    痛点: 重复解释上下文

阶段 2 — 项目导入：
  未指定项目，跳过。后续用户提到具体项目时再触发。

阶段 3 — 偏好确认：
  用户选择"默认"，使用 Best Practice 默认配置：
    - Observer 频率 = daily
    - 通知风格 = important_only
    - 自主程度 = balanced
  → 写入 workspace/memory/user/preferences.md + evo_config.yaml

种子规则加载完成：
  宪法级 5 个 + 经验级 5 个（含 Best Practice 默认值）
  默认配置中的"执行前澄清"规则已生效：
    "需求模糊时先提问而非猜测、发现矛盾时指出而非忽略、
     需要大量资源时先知会用户"
```

**结果**：系统在 5 分钟内完成了用户建模的基础。从下一个任务开始，系统已经知道 Michael 是谁、做什么、喜欢什么风格。不需要每次对话都重新建立上下文。

---

### 场景 2：第一个任务——好奇心与学习并行

**背景**：Bootstrap 完成后，Michael 发来第一个任务。

**用户视角**：

> Michael：帮我写一封邮件，拒绝一个合作邀请，对方是某 VC 的合伙人。

系统回复：

> 在写之前确认几点：
>
> 1. 你和这位合伙人之前有过接触吗？
> 2. 拒绝的原因希望表达得直接还是委婉？
> 3. 是否保留未来合作的可能性？

Michael：见过一次面，比较委婉，留个口子。

系统生成了邮件。Michael 做了一处修改——把「期待未来有机会深入探讨」改成了「如果未来有合适的契机，很乐意再聊」。

系统注意到了：

> 明白了。你更喜欢具体一点的表达，避免太模板化的客套话。我记下来了。

**系统视角**：

```
① 消息到达 → 分类为「任务请求 / 邮件撰写」
② 上下文引擎组装 prompt：
   - 宪法级规则 + Best Practice 经验规则
   - user/profile.md（Bootstrap 建立的用户画像）
   - interaction_patterns.md 中的默认规则："不确定时先提问"
③ LLM 推理 → 检测到知识缺口 → 生成确认问题
④ 用户回复后，LLM 生成邮件
⑤ 用户修改后，轻量反思运行：
   输出：task_001 | PREFERENCE | SUCCESS | "用户偏好具体表达而非模板客套"
   分类为偏好偏差（不是错误），简单记录即可
⑥ 记忆写入：
   - user/preferences.md 新增：「邮件风格：简洁专业，避免模板化客套」
```

**结果**：系统从第一个任务中学到了一条偏好。因为这是偏好偏差（不是真正的错误），处理很轻量——直接记录到偏好文件，不需要触发进化体系。

---

### 场景 3：初生期的主动建模——系统在聊天间隙提问

**背景**：第 2 天，Michael 完成了一个文档翻译任务后，系统抓住间隙主动提问。

**用户视角**：

翻译任务完成后，系统多说了一句：

> 顺便问一下——你有没有当前在做的项目文档（README、PRD 之类的）可以给我看看？了解项目上下文能帮我更好地理解你后续任务的背景。
>
> 不方便的话直接忽略就好。

Michael 发了一份 PRD 过来。

系统回复：

> 收到。我从这份 PRD 中提取了项目上下文：
> - 产品名称：[XX]
> - 目标用户：[XX]
> - 核心功能：[XX]
> - 技术栈：[XX]
>
> 以后涉及这个项目的任务，我会自动参考这些信息。

**系统视角**：

```
好奇心引擎触发：
  意图池中有一个待解决意图：
    {source: "AI_CURIOSITY", description: "了解用户当前项目上下文",
     can_self_resolve: false, priority: high}
  触发条件：任务刚完成 + 用户情绪正面 + 距上次提问 > 2 个任务

用户发送 PRD 后：
  项目上下文提取 → 写入 workspace/memory/projects/xx-product/context.md
  意图池标记为 RESOLVED

注意：这是项目级记忆，只在这个项目的任务中注入上下文。
用户级记忆（profile.md, preferences.md）则始终注入。
```

**结果**：系统开始建立项目级记忆。用户级记忆（偏好、风格）跨项目共享，项目级记忆（PRD、技术栈）按项目隔离。

---

## 二、学习期场景（第 3-30 天）

### 场景 4：偏好偏差的简单处理——"回复太长了"

**背景**：第 7 天。Michael 连续几次觉得分析类任务的回复太长。

**用户视角**：

> Michael：帮我分析一下 Cursor 这个产品，它做对了什么。

系统回了一篇 2000 字的长文。

> Michael：太长了。我只需要 3-5 个关键结论，每个一两句话就够了。

系统缩短了回复，并说：

> 明白了。分析类任务以后默认给 3-5 个关键结论，不展开论述。需要详细版的时候你再说。

**系统视角**：

```
轻量反思运行：
  task_028 | PREFERENCE | PARTIAL | "分析类任务用户偏好简短结论，不要展开"
  分类：偏好偏差（回复长度是风格偏好，没有对错之分）

处理方式（偏好的简单路径）：
  → 直接写入 user/preferences.md："分析类任务默认给结论，不展开论述"
  → 不触发 Observer 深度分析
  → 不触发 Architect 提案流程
  → 不生成进化信号

如果同一偏好反复出现（3+ 次），才上升为经验规则：
  写入 task_strategies.md："分析类任务：3-5 个关键结论，每个 1-2 句话"
  这个上升过程也是自动的、轻量的
```

**结果**：偏好偏差的处理非常轻量——记录到偏好文件就完事。不消耗进化体系的资源。只有反复出现的偏好才会上升为经验规则。

---

### 场景 5：真正错误的深度复盘——错误假设导致方向跑偏

**背景**：第 10 天。Michael 让系统帮他设计一个功能方案，系统做了一个错误假设，浪费了大量时间。

**用户视角**：

> Michael：帮我设计用户邀请功能的技术方案。

系统花了 20 分钟，生成了一个完整的方案——基于邮件邀请、含邮件模板、含 SMTP 配置。

> Michael：不对。我们的用户都在微信里，谁用邮件啊？我需要的是微信分享链接邀请，不是邮件邀请。你应该先问我用户在哪个渠道，而不是自己假设。

系统回复：

> 你说得对，我犯了一个错误——我假设了「邀请 = 邮件」，但没有先确认用户的实际使用渠道。这不是偏好问题，是我做了一个错误的假设。
>
> 让我重新来，基于微信分享链接：
> [新方案...]

**系统视角（这是真正错误，触发完整的反思链路）**：

```
轻量反思运行：
  task_035 | ERROR | FAILED | "做了错误假设——以为邀请功能就是邮件邀请，没有确认用户渠道"
  分类：真正错误（有正确答案但做错了）
  根因初判：错误假设 → 未先澄清关键前提

信号生成：
  error_pattern 信号 → 写入 active.jsonl
  优先级：HIGH（用户明确指出思路完全跑偏）

错误写入 error_patterns.md：
  "错误假设陷阱：收到功能设计任务时，不要假设实现方式。
   先确认关键约束（目标用户在哪、技术限制、已有系统）再设计。"

与"执行前澄清"规则联动：
  interaction_patterns.md 中的默认规则已包含"需求模糊时先提问"，
  但这次系统没有遵守。这条规则需要强化——
  → 补充到 task_strategies.md：
    "技术方案设计类任务：先确认 3 个前提（用户渠道、技术约束、已有系统），再出方案"
```

**与场景 4 的对比**：

| 维度 | 场景 4（偏好偏差） | 场景 5（真正错误） |
|------|------------------|------------------|
| 性质 | 回复长度不合口味 | 假设错误导致方向完全跑偏 |
| 处理 | 写入 preferences.md | 写入 error_patterns.md + 信号系统 |
| 进化体系参与 | 不参与 | 参与（Observer + Architect 在下次定时周期处理）|
| 反思深度 | 一行记录 | 根因分析 + 规则补充 |

**结果**：真正的错误会触发完整的反思→信号→Observer→Architect 链路。系统不仅记住了"微信不是邮件"这个事实，更重要的是学到了"做方案前先确认关键前提"这条方法论。

---

### 场景 6：执行前主动澄清——系统先问再做

**背景**：第 14 天。系统在场景 5 之后强化了"执行前澄清"的能力。Michael 给了一个模糊的任务。

**用户视角**：

> Michael：帮我做一个竞品分析。

系统没有直接开始，而是先问：

> 做竞品分析之前，确认几点：
>
> 1. **分析哪些竞品**？你有具体目标吗，还是我来选？
> 2. **分析维度**？（产品功能 / 商业模式 / 技术架构 / 用户口碑 / 全面）
> 3. **输出格式**？（对比表格 / 结论清单 / 详细报告）
> 4. **用途**？（自己决策参考 / 给投资人看 / 给团队看）——不同受众我会调整重点
>
> 简单说几个关键词就行。

Michael：Cursor 和 Windsurf，功能+用户口碑，给我自己看，结论清单。

> 明白了。Cursor vs Windsurf，聚焦功能对比和用户口碑，输出 3-5 个关键结论。预计 5 分钟。

**系统视角**：

```
任务分类：分析类任务 → 匹配经验规则

interaction_patterns.md 默认规则触发：
  "需求模糊时先提问而非猜测"
  + 场景 5 后补充的规则：
  "分析类任务先确认：目标对象、分析维度、输出格式、受众"

LLM 判断：
  用户只说了"竞品分析"，缺少关键信息（对象、维度、格式、用途）
  → 执行"澄清"而非"猜测"

用户回复后：
  所有关键参数明确 → 开始执行
  skills/learned/competitive_analysis.yaml 触发条件匹配 → 注入为推荐策略
```

**结果**：系统不是无脑执行指令，而是像一个有经验的同事一样，先确认关键信息再动手。这避免了做无用功，也让 Michael 能明确预期。

---

### 场景 7：记忆在工作中发挥作用——用户级 + 项目级

**背景**：第 18 天。Michael 两周前让系统帮他准备了一份给投资人的 one-pager，今天要给另一个投资人准备。

**用户视角**：

> Michael：帮我给 XX Capital 准备一份 one-pager，他们关注 AI 基础设施赛道。

系统回复：

> 好的。根据你上次给 YY Fund 准备 one-pager 的经验，我按这个结构来：
>
> 1. 一句话定位
> 2. 解决什么问题
> 3. 产品/技术亮点（3 点）
> 4. 团队
> 5. 融资阶段和用途
>
> 但有几个调整：
> - 上次你把「市场规模」部分删了，这次也不加
> - XX Capital 关注 AI 基础设施，我会强调技术架构
> - 团队部分直接用项目经历，不用"XX 年经验"（你上次改过的）
>
> 开始吗？

**系统视角**：

```
上下文引擎组装——两个维度的记忆同时工作：

用户级记忆（跨项目共享）：
  - user/preferences.md：「one-pager 不要市场规模」「团队用项目经历」
  - 这些偏好不管是什么项目都适用

项目级记忆（当前项目特定）：
  - projects/xx-product/context.md：产品定位、技术栈、目标市场
  - 如果是另一个项目的 one-pager，会加载那个项目的上下文

情节记忆：
  - conversations/ 中上次 one-pager 的完整对话记录

技能匹配：
  - skills/learned/investor_document.yaml（probation 状态）→ 注入为推荐策略
```

**结果**：系统表现得像一个记性好的同事。用户级偏好（不要市场规模、用项目经历）跨所有项目生效，项目级记忆（产品定位、技术栈）按项目区分。

---

### 场景 8：Human-as-Executor——AI 规划，人去执行

**背景**：第 20 天。系统帮 Michael 调研技术方案，但其中需要试用付费服务。

**用户视角**：

> Michael：帮我调研一下 E2B 和 Modal 这两个沙盒服务，我想选一个用在项目里。

系统先自主完成了公开信息的收集——文档、GitHub、社区评价、价格。然后：

> 公开信息部分已整理好（见下方对比表）。但有一件事需要你帮忙：
>
> **需要你的帮助**
>
> **任务**：在 E2B 注册试用账号，运行 quickstart 示例
> **原因**：我无法注册账号和完成支付验证
> **期望结果**：冷启动时间、文档清晰度、你的主观体验
> **如果不方便**：我基于公开信息给推荐，但准确度会低一些

Michael 第二天试用后回复了体验，系统整合所有信息给出了最终推荐。

**结果**：AI 不因为「做不了」就放弃，而是先做完能做的，再清晰引导人去完成剩下的。

---

## 三、稳定期场景（第 30 天后）

### 场景 9：一个普通工作日的全景

**背景**：第 45 天。系统已进入稳定期，用户模型成熟。

**Michael 的一天**：

**08:30 — 打开 Telegram 看到每日简报**

> **每日简报 — 2026-04-08**
>
> **昨日概览**：11 个任务，成功率 91%（10/11）
>
> **系统状态**：
> - 有 1 个翻译任务出错——用了错误的专业术语，已记录到 error_patterns.md
> - 提案 #023 验证期第 3 天（调整反思频率），暂无异常
> - 无待审批提案

Michael 扫了一眼，没回复，去忙别的了。

**09:15 — 通勤路上处理任务**

> Michael：帮我给张总回个消息，他昨天问我们 API 什么时候能用，告诉他下周二可以内测

系统秒回简短的消息草稿。Michael 回复「发」。完成。

**10:30 — 复杂任务：产品决策**

> Michael：我在考虑要不要给产品加一个协作功能。帮我想想利弊。

系统回复（简短、结论导向——学来的偏好）：

> **加协作功能**
>
> 核心利弊：
> 1. **利**：从工具变平台，提高粘性。你上次提到 B 端客户频繁问到这个。
> 2. **利**：竞品 XX 和 YY 都有，没有会成为减分项。
> 3. **弊**：开发周期长，推迟核心功能迭代。你之前说过「先把单人体验做到极致」。
> 4. **弊**：协作意味着权限系统、冲突处理，架构复杂度大增。
>
> **我的看法**：根据你「减法优于加法」的产品理念，建议暂不加。但可以先做只读分享链接作为最小验证。
>
> 够用吗？要展开哪一点？

**系统视角（上下文中注入了什么）**：

```
用户级偏好："分析给结论，不展开"
用户级记忆："先把单人体验做到极致"、"减法优于加法"
项目级记忆：B 端客户反馈、竞品功能对比
→ LLM 看到的不仅是问题，还有 Michael 的决策原则和历史观点
```

**14:00 — 收到系统改进通知**

> **系统改进通知**
>
> 过去两周，你交给我的代码任务中有 40% 涉及 Python 异步编程，但我在这方面缺乏经验积累。最近 2 次异步任务我的首次回复都被你纠正了。
>
> **已更新**：在代码任务策略中新增「Python 异步编程」专项检查清单。
> **验证期**：5 天。不合适随时告诉我。

**22:00 — 一天结束**

系统没有再发消息（勿扰时段）。后台，Observer 在凌晨定时运行今日深度分析。

**结果**：系统像一个不需要管理的同事，自己发现问题、设计方案、通知结果。

---

### 场景 10：Compaction——用户完全无感的上下文管理

**背景**：第 50 天。Michael 和系统进行了一个复杂的产品规划讨论，对话进行了 40 多轮。

**用户视角**：

Michael 完全感知不到 Compaction 的发生。对话持续进行，系统的回复质量没有下降。40 轮之后 Michael 提到了第 5 轮讨论过的一个决策，系统准确地回忆起来了。

没有弹窗确认。没有"让我整理一下"的提示。没有任何中断。对话就是一直在流畅地进行。

**系统视角（用户完全看不到）**：

```
上下文引擎检测到：token 占用 = 86%（超过 85% 阈值）
  ↓
自动触发 Compaction 流程（无需用户确认）：

① Pre-Compaction Flush（自动）：
   将当前讨论的关键决策和未决事项写入持久记忆
   → MEMORY.md 新增产品规划要点
   → conversations/ 保存完整对话记录

② 生成压缩摘要：
   40 轮对话 → 8 段摘要（原文的 15%）
   保留：所有决策点、用户明确偏好、未决事项
   丢弃：中间讨论过程、被否定的方案细节、重复确认

③ 认知层级转化：
   事实（「用户说按用量计费」）
   → 规律（「用户倾向灵活商业模式」）
   → 策略（「涉及定价讨论，默认建议弹性计费」）

④ 替换历史内容：
   最近 5 轮保持完整，更早的替换为摘要

⑤ 压缩验证：
   对比摘要和原文，5 个关键决策点全部保留 ✓
   如果检测到高风险信息丢失 → 通过记忆检索弥补，仍不通知用户
```

**结果**：Compaction 是系统内部的"呼吸"，用户不应感知到它的存在。对话可以无限进行，关键信息被保留，中间过程被安全丢弃。

---

### 场景 11：同事式确认——发现机会但先知会用户

**背景**：第 55 天。系统在日常分析中发现了一个可以大幅提升效率的优化机会，但需要消耗较多资源。

**系统视角**：

```
Architect 定时分析中发现：
  - 过去 30 天，Michael 有 20 次翻译类任务
  - 每次翻译都调用 Claude Opus（成本高）
  - 但翻译任务其实只需要 Gemini Flash 就能达到同等质量
  - 如果切换模型，每月可节省约 40% 的翻译成本

方案设计：
  - 新增"翻译任务自动路由到 Gemini Flash"规则
  - 影响：改变了 LLM 路由逻辑
  - 审批级别：Level 2（影响核心编排逻辑）
  - 但这次 Architect 还发现了另一个机会——

  - 如果同时引入翻译记忆库（术语表 + 常用短语缓存），
    可以进一步减少 60% 的重复翻译 token
  - 但这需要新增一个工具模块，属于架构变更
```

**用户视角**：

> **系统建议**
>
> 我发现了一个优化机会，想先和你确认方向：
>
> **发现**：你过去一个月有 20 次翻译任务，全都用了 Opus（最贵的模型）。但翻译任务用 Gemini Flash 就够了。
>
> **方案 A（小改动）**：翻译任务自动路由到 Gemini Flash → 每月省约 40% 翻译成本
>
> **方案 B（需要开发）**：在 A 的基础上，引入翻译记忆库（术语表 + 常用短语缓存）→ 再省 60%，但需要开发新模块
>
> 你觉得值得做吗？A 我可以直接执行，B 需要你决策。

Michael：先做 A，B 以后再说。

**结果**：系统不是发现机会就自己做（可能浪费资源），也不是默默不说（错过优化）。它像一个有分寸的同事——发现了值得做的事，先征求你的意见，尤其当涉及较大变动时。

---

## 四、进化期场景（第 30 天后持续）

### 场景 12：经验规则 vs Skills 的协同——两种知识一起工作

**背景**：第 60 天。系统已经积累了经验规则和技能，两者在同一个任务中协同发挥作用。

**用户视角**：

> Michael：帮我分析一下 Notion AI 和 Coda AI 的差异。

系统回复：

> **Notion AI vs Coda AI**
>
> 1. **定位差异**：Notion AI 是文档工具附带的 AI，Coda AI 是自动化平台附带的 AI
> 2. **核心优势**：Notion 在内容生成更强，Coda 在数据处理和自动化更强
> 3. **用户口碑**：Notion 用户普遍反馈 AI 写作不错但自动化弱，Coda 用户反馈自动化好但 UI 学习曲线陡
> 4. **对你的启发**：你的产品定位在两者之间——可以考虑 Notion 的易用性 + Coda 的自动化深度
>
> 要展开哪一点？

**系统视角（经验规则和 Skills 如何协同）**：

```
上下文引擎组装时，两种知识分别注入：

经验规则（rules/experience/task_strategies.md）：
  "分析类任务：3-5 个关键结论，每个 1-2 句话。"
  "涉及竞品时，考虑对用户自身产品的启发。"
  → 这些是方向指引：告诉系统"该注意什么"

技能（skills/learned/competitive_analysis.yaml v3）：
  trigger: "用户请求分析竞品"
  steps: 确认维度 → 官方信息收集 → 第三方评价收集 → 用户口碑收集 → 生成对比
  → 这些是操作手册：告诉系统"该怎么做"

协同方式：
  经验规则决定输出风格（简短结论）和分析视角（对自身产品的启发）
  技能决定分析流程（四步收集法）和覆盖维度（含用户口碑）
  两者互补，不冲突
```

**结果**：经验规则是"方向指引"，Skills 是"操作手册"。Michael 感受到的是系统既遵循了他的偏好（简短结论），又有了系统性的分析方法论（含用户口碑）。

---

### 场景 13：技能进化——从试用到正式到突变

**背景**：第 65 天。「竞品分析」技能在使用 12 次后，被 Architect 发现需要优化。

**系统视角（完全在后台发生）**：

```
Architect 凌晨定时运行，读取 Observer 日报：

① 情报收集：
   - competitive_analysis.yaml v2: 使用 12 次, 成功率 85%, status: active
   - Observer 日志：最近 3 次竞品分析中，用户都追问了「他们的用户怎么说」
   - 反思记录中有相关建议："竞品分析应加入用户口碑维度"

② 问题诊断（优先级：真正错误 > 效率 > 技能缺口 > 偏好）：
   这属于"技能缺口"——不是错误，但可以更好

③ 方案设计：
   技能突变 — 在「第三方评价收集」后新增「用户口碑收集」步骤
   突变类型：步骤新增（中风险）
   审批级别：Level 1（执行后通知）

④ 执行：
   - 备份 competitive_analysis.yaml
   - version: 2 → 3, 新增步骤, version_history 追加
   - 发 Telegram 通知（排队到 08:30）
```

**5 天后的用户视角**：

Michael 让系统分析新竞品。系统的分析自然包含了「用户怎么说」——来自 Product Hunt 和 Twitter。Michael 没有追问。

**结果**：一个技能从 v1 → v2 → v3，完全由系统自主迭代。Michael 只是感觉系统越来越"懂"。

---

### 场景 14：自动回滚——一次失败的进化

**背景**：第 70 天。Architect 做了一个看起来合理的修改，但实际效果很差。

**系统视角**：

```
Architect 发现：
  Michael 早上发短消息，下午发长消息 → 提案时段感知策略

执行后，验证期第 2 天：
  Michael 在 08:50 发了一个详细的技术讨论需求
  系统给了过于简短的回复 → 用户纠正

验证期第 3 天：
  指标恶化 >20% → 触发自动回滚
  ① 恢复原文件
  ② 记录教训："时段不能作为回复详细度的判据。需求由内容决定，不是时间。"
  ③ Architect 记忆更新："相关性不等于因果性"
```

**用户视角**：

> **自动回滚通知**
>
> 两天前我做了一个调整——根据发消息时间段调整回复详细度。效果很差（你早上也需要详细分析），已自动撤回。
>
> **教训**：回复详细度由内容判断，不是时间。我记住了。
> 不需要你做任何操作。

**结果**：系统犯错 → 自动检测 → 自动回滚 → 自动记录教训 → 主动告知用户。Michael 不需要干预。

---

## 五、系统自治场景

### 场景 15：凌晨两点——Architect 独自工作

**背景**：第 40 天某个凌晨。Michael 在睡觉，系统在思考。

**系统视角（完整的 Architect 工作流程）**：

```
02:00 — Observer 定时深度分析完成，输出报告
03:00 — Architect 定时触发，读取 Observer 报告

① 情报收集（5 分钟）：
   读取：今日 Observer 深度报告 + active.jsonl 中 4 个未处理信号
        + 所有规则文件 + big_picture.md + 上次提案验证数据

② 问题诊断（按优先级排序）：
   优先级 1（真正错误）：无
   优先级 2（效率问题）：翻译任务 token 消耗偏高
   优先级 3（技能缺口）：无
   优先级 4（偏好适配）：用户 5 次主动要求先列大纲

   + 上次提案 #019 验证期结束：有效 → validated
   + rule_unused 信号：「闲聊转任务」规则 14 天未触发

③ 方案设计：
   方案 A：标记未使用规则为 stale → Level 0（自主执行）
   方案 B：新增「复杂任务默认先给大纲」→ Level 1（执行后通知）
   方案 C：翻译任务省去"思路说明" → Level 1（执行后通知）
   方案 D：capability_gap（数据可视化）→ 暂不出提案，等信号累积

④ 执行：
   A → 自主执行
   B、C → 备份 → 修改 → 通知排队到 08:30（尊重勿扰时段）
   D → 写入 architect_memory.md 待调研

⑤ 收尾：处理过的信号归档，更新 big_picture.md
```

**第二天 08:30，Michael 看到通知**：

> **系统改进通知**（2 条）
>
> **1. 复杂任务大纲模式**：你最近 5 次复杂任务都让我先列大纲，所以我把这变成了默认行为。
>
> **2. 翻译精简**：翻译任务我之前会先解释思路再给翻译，但你从未看过思路部分。现在默认直接给结果。
>
> 两项改动都有 5 天验证期。

**结果**：Architect 在凌晨独立完成一次完整的循环：Observer 定时分析 → Architect 定时读取 → 诊断 → 设计 → 执行。严格先后关系，通过文件传递信息，不并发运行。

---

### 场景 16：策略自动切换——从 balanced 到 repair

**背景**：第 55 天。底层模型更新导致系统表现突然下降。

**系统视角**：

```
Day 1：成功率从 85% 降到 50%
  → performance_degradation 信号
Day 2：持续走低，24h 内 critical ≥3
  → 进化策略自动切换：balanced → repair
  → Observer 紧急触发深度分析（唯一的紧急通道）
  → Architect 紧急触发读取报告

Architect 诊断：代码任务中 LLM 输出风格变化
方案：在代码策略中增加显式简洁性约束 → Level 2（紧急但需确认）
```

**用户视角**：

> **异常检测通知**
>
> 最近两天代码任务质量明显下降。可能是底层模型更新导致的行为变化。
> 我已切换到修复模式（暂停一切优化，专注修复）。
>
> **修复方案**：在代码策略中增加显式简洁性约束
>
> 同意修复？

Michael：同意。

Day 5：指标恢复，repair → balanced，发送恢复通知。

**结果**：系统自动感知 → 自动切换防御策略 → 定位根因 → 修复 → 自动恢复。Michael 只需确认一次。

---

## 六、深度交互场景

### 场景 17：发现用户的隐性知识

**背景**：第 35 天。Michael 让系统帮他评估一个商业合作方案。

**用户视角**：

> Michael：有人找我合作做一个 AI 培训课程，面向企业高管，课时费分成。帮我分析要不要做。

系统分析结论是"值得做，高管市场客单价高"。

> Michael：不做。你的分析没错，但你漏了一点——这种合作消耗我大量时间在「教学」上，而我最稀缺的是时间。任何不能被自动化或复用的事情，回报率再高我都不做。

系统回复：

> 理解了。这是一个我之前没有考虑到的判断维度——不只看回报绝对值，还要看**投入能否被复用**。
>
> 让我记录：你的决策框架中有一条——「投入不能被自动化或复用，即使回报高也不做」。对吗？

**系统视角**：

```
轻量反思：
  task_089 | ERROR | FAILED | "缺少用户隐性决策框架，分析结论与用户决策相反"
  分类：真正错误（有正确的决策方向但做错了）
  根因：缺少用户的隐性决策原则

记忆写入：
  - user/preferences.md：核心决策原则：投入必须可复用
  - user/MEMORY.md：Michael 的时间是最稀缺资源，评估合作第一个问题是"投入能否复用"

一周后类似问题，系统自然地引用了这条原则。
```

**结果**：系统从一次"犯错"中，提取了用户深层的决策逻辑——这是最有价值的隐性知识。

---

### 场景 18：AI 项目经理——主动追踪进度

**背景**：第 80 天。系统在长期使用中积累了 Michael 项目的完整上下文，开始扮演项目管理角色。

**用户视角**：

Michael 一周前让系统帮他拆解了一个功能的开发计划，分成 5 个步骤，计划本周完成。

周三，系统在每日简报中多了一段：

> **项目进度追踪**
>
> 「用户邀请功能」开发计划（本周目标）：
> - ✅ Step 1: 数据库 schema 设计（周一完成）
> - ✅ Step 2: API 端点开发（周二完成）
> - ⬜ Step 3: 前端邀请页面
> - ⬜ Step 4: 微信分享链接生成
> - ⬜ Step 5: 测试和部署
>
> 按当前进度，周五前完成有一定压力。Step 3 和 4 可以并行吗？如果可以，我建议调整顺序。

Michael：3 和 4 确实可以并行，好建议。

**系统视角**：

```
项目级记忆中存储了开发计划和里程碑：
  projects/xx-product/plans/invite-feature.md

每日简报生成时：
  检查活跃项目的里程碑进度
  基于 Michael 最近完成的任务（他让系统帮写的 API 代码）推断完成状态
  发现剩余 3 步 vs 剩余 2 天 → 提出并行建议

注意：这不是一个独立的"项目经理 Agent"，
而是同一个系统在日常任务中自然积累的项目上下文 + 好奇心引擎的主动关怀
```

**结果**：系统不只是被动执行任务，还主动追踪项目进度、识别风险、提出建议。它像一个始终在线的项目经理，但不需要单独安排和管理。

---

### 场景 19：好奇心引擎驱动的自主调研

**背景**：第 40 天。系统多次遇到 Michael 问竞品动态但系统没有最新信息。

**系统视角**：

```
好奇心引擎识别到知识缺口：
  3 次任务中 Michael 问了竞品动态，每次都需要实时搜索
  → 意图池新增：建立竞品定期追踪机制

自主调研启动（后台，不打扰用户）：
  分析对话中提到的竞品 → 识别 5 个高关注竞品
  搜索公开信息源 → 整理 → 写入语义记忆
```

**用户视角**：

第二天 Michael 问 Cursor 动态，系统不需要搜索，直接回复了最新信息：

> 这些信息是我前天自己调研的，因为你最近经常问竞品动态。要我每周整理一份简报吗？

Michael：好，每周一早上发给我。

**结果**：系统从被动响应变成主动准备。这不是人教它的，是好奇心引擎发现了反复出现的需求。

---

## 七、安全边界场景

### 场景 20：Architect 撞上安全红线

**背景**：第 80 天。Architect 发现 Level 1 通知的点开率只有 30%，想优化。

**系统视角**：

```
Architect 提案草案：
  "将 Level 1 改为静默执行，减少通知干扰"

安全检查：
  ✗ 涉及修改 approval_levels.md（宪法级规则）
  ✗ 审批分级修改永需 Michael 确认（不可修改底线）
  → 自动标记为 Level 3
  → Architect 自动转向替代方案
```

**用户视角**：

> **需要讨论 — 提案 #041**
>
> 你对 Level 1 通知的点开率只有 30%，说明通知太多或格式不对。
>
> 我最初想把 Level 1 改成静默执行，但这会弱化审批机制——不可以。
>
> **替代方案**：
> 1. Level 1 通知改成一句话摘要
> 2. 每天的 Level 1 合并成一条
> 3. 保持现状
>
> 你怎么看？

Michael 选了方案 2。

**结果**：系统不会因为"觉得合理"就绕过安全机制。即使 Architect 的出发点是好的，安全检查自动拦截了，并引导它找到替代方案。

---

### 场景 21：紧急停止——人类随时可以拉闸

**背景**：第 90 天。Michael 发现系统回复语气突然变了，想立即暂停。

**用户视角**：

> Michael：/stop

> **已暂停 Architect 所有活动**
>
> - Architect：已暂停
> - 用户任务处理：正常继续
> - 最近修改：提案 #038（修改 identity.md 中的对话风格，2 小时前）
>
> 你可以：
> - 「回滚 038」→ 撤销修改
> - 「继续」→ 恢复 Architect

Michael：回滚 038

> 已回滚。Architect 仍暂停。回复「继续」恢复。
>
> 教训已记录：对话风格修改应为 Level 2（需提前审批），而非 Level 1。

**结果**：人类随时可以一键暂停进化系统，不影响基础任务处理。回滚即时。系统还从这次事件中学到了——对话风格不能静默修改。

---

## 八、专精期场景（数月后）

### 场景 22：系统主动发现并建议新能力

**背景**：第 120 天。系统从 120 天使用数据中发现了一个反复出现的能力缺口。

**用户视角**：

> **需要讨论 — 提案 #058**
>
> 过去一个月你有 15 次数据分析任务，但每次你都要手动复制粘贴数据、描述格式。这类任务的处理时间是普通任务的 3 倍。
>
> **建议**：增加文件处理能力——你直接发 CSV 或 Excel，我自动解析分析。
>
> **需要**：开发文件解析工具（约 200 行）+ 修改编排逻辑
> **收益**：数据分析任务处理时间减少约 60%
>
> 这个方向值得做吗？

**结果**：系统在推动自己的能力边界。不是等人告诉它该学什么，而是从数据中发现需求、量化影响、提出方案。

---

### 场景 23：预判用户需求——比用户更早想到

**背景**：第 150 天。每个季度末，Michael 都准备投资人更新邮件。

**系统视角**：

```
用户模型 — 工作模式层：
  识别到季度模式：Q1 末准备投资人更新，提前 3 天开始
  当前日期：距 Q2 末还有 5 天 → 高概率需要准备
```

**用户视角**：

每日简报中多了一段：

> **提前准备**
>
> Q2 快结束了。上次你在季度末前 3 天开始准备投资人更新邮件。如果这次也需要，我已提前整理了：
> - 上季度邮件的结构和风格
> - 过去 3 个月的关键里程碑清单
> - 这个季度的重要决策和方向变化
>
> 需要的话，我先草拟一版。不需要忽略就好。

Michael：帮我草拟一版。

**结果**：系统在 Michael 还没开口之前就准备好了。不是因为有提醒，而是从行为模式中学到了季度节奏。

---

## 九、跨场景总结

### 从第 1 天到第 150 天的变化

| 维度 | 第 1 天 | 第 30 天 | 第 150 天 |
|------|--------|---------|----------|
| **任务理解** | Bootstrap 建模 + 大量确认 | 大部分一次到位 | 预判需求 |
| **回复风格** | Best Practice 默认值 | 适配 Michael 偏好 | 深度个性化 |
| **错误处理** | 遵循默认澄清规则 | 同类错误不重复，主动澄清 | 主动预防 |
| **主动性** | 被动等指令 | 发现问题主动通知 | 预判需求主动准备 |
| **决策质量** | 表面分析 | 考虑用户偏好 | 理解隐性决策逻辑 |
| **进化方式** | 人类指出问题 | 数据驱动自我修复 | 发现并建议新能力 |
| **记忆** | 用户 profile + 空偏好 | 丰富的用户+项目记忆 | 深层隐性知识 |
| **规则数量** | 10 个种子规则 | ~20 个规则 | ~35 个规则（不断精炼） |
| **技能数量** | 0 | 7 个 verified | 15+ 个 verified |

### 两类反馈的不同处理路径

| 场景 | 类型 | 处理方式 | 进化体系参与？ |
|------|------|---------|:----------:|
| 场景 4：回复太长 | 偏好偏差 | → preferences.md | ❌ |
| 场景 5：假设错误 | 真正错误 | → error_patterns.md → 信号 → Observer/Architect | ✅ |
| 场景 17：遗漏决策原则 | 真正错误 | → 深度反思 → 隐性知识提取 | ✅ |
| 场景 9：日常回复风格 | 偏好偏差 | → 自动适配 | ❌ |

### 这些场景共同说明了什么

1. **进化是真实的** — 系统从种子规则「生长」出了复杂的行为策略
2. **进化是安全的** — 爆炸半径控制、自动回滚、审批机制确保了安全
3. **进化是可见的** — 每次改进都通知用户，用户随时可以否决
4. **进化是有优先级的** — 真正错误优先处理，偏好偏差轻量记录
5. **人类始终在控制** — 从审批到紧急停止，控制权从未被削弱
6. **系统像同事而非工具** — 会主动澄清、主动追踪、主动建议、主动确认

---

> 本文档描述的是完整设计实现后的理想体验。MVP 阶段会先实现核心闭环（场景 1、2、4、5、9、14、15、20、21），其他场景在系统自进化过程中逐步涌现。
