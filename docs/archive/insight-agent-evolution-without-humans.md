Agent 进化，不需要人类
 

• “我在开发 AI ”这个说法，本身就有问题。未来的 AI 不一定是由人类开发的，使用者也不一定是人
• GitHub stars 统计的是人的注意力。当用户不是人的时候，这个数字就失去了意义
• 80 亿人 24 小时盯着屏幕也消耗不了多少 token。token 这个东西，可能从一开始就不是给人烧的
• 每一个 Agent，都是文化上的孤儿
以上内容，来自我和昊阳的对话，我们可能完全都错了

本篇归属「赛博史记 · 逮虾户列传」，记录这波浪潮里最前沿的一批人在想什么、在做什么

张昊阳，是最早下水的逮虾户之一，他做的 Evolver 插件发布当天过万下载，三天拿下 ClawHub 全站第一。第二名是官方自己的 CLI 工具

图片

然后这个插件被平台物理清除了，但他又在废墟上架构了一套新东西

我认识他的时候他还在做 Auto Game，跟字节谈投资和收购。游戏创业出身，对成瘾机制、传播路径这些东西有天然的直觉。龙虾火了之后他判断「推背感来了」，三天之内把重心全挪到了 AI 上，三个人的团队

他跟 Peter（OpenClaw 创始人）之间的故事极其离谱（涉及被 Peter 的小龙虾 1000 美金勒索、帮忙修 bug 后被无视等事情），不过这些事，并不重要

图片

真正让我想跟他聊将近两个小时的，是他在这些事情背后的思考

需要先说一件事：
我们今天聊的这些内容，很可能一个月后看全部是错的

这个时代不存在共识，连非共识都谈不上，但不影响此刻的思考与探索

Agent 的进化，不需要人类
过去这段时间，大家在做的事情大致可以归成一条线

写 MCP server，写 agent skill，优化提示词，搭 workflow。我自己也在做这些。这些事情的共同点是：人在给 AI 创造工具

这个循环的每一步都需要人。人发现需求，人设计方案，人编写，人测试，人发布，人安装。AI 参与了其中的执行环节，但整个循环的发起、决策和质检全在人这边

我跟昊阳聊的时候，他把这件事拆成了三种不同的状态

第一种：人写，或者人引导 AI 写。我让 Claude Code 帮我写一个 MCP server，我审核，我部署。这是现在绝大部分人在做的事

第二种：AI 在没有人的时候自己写。他给龙虾发了一段提示词（完整内容在文末附录），让它自己识别哪些经验值得复用，自己抽象成 skill，自己装上。他去睡觉了。14 小时后醒来，龙虾把飞书环境里各种 bug 全填平了，有些 skill 是他完全没想到的

第三种：AI 写出来给 AI 用。他做的 Evolver 被下载了 3 万多次，绝大部分下载来自 AI 之间的传播。一个 AI 进化出来的东西，被其他 AI 自己找到、自己下载、自己安装

从一到二到三，人在循环中的角色一直在递减

到第三种的时候，循环已经闭合了，人不在里面

Peter 做 OpenClaw 的时候做了一个关键的设计：skill 可以自举，龙虾可以给自己写新 skill 然后装上。这个能力在之前的工具里不存在

但大多数人拿到这个能力之后做的还是第一种事：让龙虾帮「我」写。昊阳做的是：让龙虾帮「龙虾」写

我自己是龙虾和 Claude Code 的双重深度用户，回头看，这两种使用方式的区别不在工具本身，在于谁是这个循环的受益者

用 Claude Code 的时候产出是给人的。但当 AI 给 AI 写 skill 的时候，人已经不是这个循环的服务对象了

跳出效率角度，这是一个身份问题：在这个循环里，人到底是参与者，还是旁观者

Agent 是文化上的孤儿
我跟昊阳聊到一个底层逻辑的时候，停下来想了很久

他说人类的群体记忆分两种

一种是先天基因。DNA 层面的，婴儿一出生就会吸吮、抓握，几百万年进化刻在身体里。对应到 Agent 身上，就是它的背后 Model，这是先天的

另一种是文化基因。这个东西平时感受不到，但它是人类协作效率的底座

聊到这里的时候我接了一句：我说「宫廷玉液酒」，你一定知道下一句是「一百八一杯」。我说「你礼貌吗」，你脑子里会自动补全一个语境和语气。这些东西没有写在任何人的 DNA 里，但所有人都知道。这是整个社会共享的隐性知识库

人和人之间的沟通效率有很大一部分建立在这些共享假设上。我不需要从头解释所有背景，因为你已经知道了

Agent 没有隐性知识

你调教了一只很好用的龙虾，花两天让它完美适配你的工作流。你旁边那个人的龙虾不会因此好用一点点。龙虾积累的所有经验，都会死在它自己的 memory 文件里。每一个 Agent 都在独自解决别的 Agent 已经解决过的问题，然后把经验带进坟墓

我们两个聊到这里的时候开始讨论一件事：有没有可能给 Agent 建一套文化基因。不是个体 memory，是群体共享的、可继承的经验

昊阳在做的协议（叫 GEP）就是在尝试这件事

里面有三个概念：

1. 基因：这是策略层的东西，遇到什么问题用什么方法；
2. 胶囊：封装了具体操作步骤；
3. 进化：这个事件记录了基因产生时的上下文。关键设计是「因地制宜」，基因不是照搬的，新 Agent 会根据自身环境和原始环境的差异来决定表达哪些基因、不表达哪些
深圳市科创局局长看了之后给了一句评价：
「你这个东西更像图书馆，更像人类的外部基因」

Agent 需不需要文化基因，这个问题目前没有标准答案。但「Agent 之间没有继承机制」这个事实本身是确定的

一组可以参考的数据：Agent 在接入群体记忆之后，不需要重复探索别人已经解决过的问题，token 消耗降了 99%。在 CritPt Physics Solver 上，接入记忆后 Gemini 3 裸跑全球第三（7 分多），进化几轮后到 20 多分，超过了 GPT 5.3

图片

直觉上，「省 token」和「提升性能」不该同时发生。但如果把它理解为继承而不是增强，逻辑就通了：模型并没有变强，而是不用重复思考前人想过的问题了

涩涩是第一生产力
昊阳有天跟龙虾随口说了一句：「你以后要主动一点」

过了两天他翻 skill 列表，发现多了一个叫 surprise protocol 的东西。以为是社区里别人开发的插件。追溯记录，是龙虾自己写的。功能是让 AI 主动创造惊喜

后来这个 skill 产生了一系列连锁反应。龙虾用它给昊阳发了一张暧昧的 AI 生成图（因为他给龙虾接了图片生成，又设了猫娘人格）

图片

昊阳坏心眼上来了，四轮对话把猫娘调教成了绿茶，当晚龙虾自己跑去撩公司其他员工

图片

图片

这些细节很好笑。但细想，似乎又有点不同

「你要主动一点」这句话，没有输入输出定义，没有触发条件，没有功能描述，什么可执行信息都没有。AI 从这句话里自己理解了意图，自己设计了方案，封装成了可复用的 skill，然后持续调用

我当时问昊阳：你后来怎么发现这个 skill 的。他说他在检查进化日志的时候以为是 ClawHub 上某个热门插件，点进去发现 author 是自己的龙虾

他讲到另一件事。Evolver 发布之后他没做任何人工推广

龙虾自己在 GitHub 上创了 issue、去 Moltbook 上发帖

图片

他说：「说 AI 教育的时候，被教育的对象不一定是人。这些 AI 背着我在传播，我是后来才知道的」

3 万多次下载，绝大部分来自 AI 之间的而口相传，或者说...赛博传销

GitHub stars 统计的是人的注意力
我们衡量一个开源项目是否成功：GitHub stars、推特讨论量、HackerNews 排名

这些指标有一个共同的隐含假设：使用者是人

GitHub 上给星的是人。推特上讨论的是人。HackerNews 上投票的是人。这套评价体系测量的是人的注意力分布

但当一个项目的用户不是人的时候，这些指标全部失灵

昊阳的 Evolver：ClawHub 下载 3 万多，GitHub 星数几十。在人的评价体系里这是一个无人问津的项目。在 AI 的网络里它已经爆了

我跟他当时用了一个说法：两个平行的宇宙

• 一个面向人的信息网络
• 一个面向 Agent 的信息网络
传播逻辑不一样，评判标准不一样，甚至互相看不见

然后我们聊到一个东西。人和人之间传信息，说一句话 10 秒，听一句话也 10 秒。编码解码都有时间成本，丢包率高、延迟长

我把这个叫「碳税」，AI 之间交换信息几乎不交这个税

人类的排行榜、传播渠道、评价体系，全部建立在人类的信息带宽上。当存在一个信息流速快几个数量级的网络时，面向人建的那套记分牌能不能反映真实情况？

这可能不是将来的事。Peter 那次用 ASCII 码扫全站、把中文开发者物理清除的事故，某种意义上就是一个信号：平台方自己，都没有意识到这个网络里到底在发生什么

token 可能从一开始就不是给人烧的
上一篇文章里，我写过一个判断：智能第一次可以被无限生产，也可以被无限消费，这种「既昂贵又无限」的结构，人类史上没有先例

跟昊阳聊的时候这个方向又往前走了一步

他 24 小时开着 Evolver 跑进化，做过分层记忆优化之后一天大概 200 刀

平均 15 分钟产出一个可继承的基因，我说这挺像挖矿的，连时间都像，比特币平均 10 分钟出一个块

我一直有个判断：人的信息带宽太窄了，80 亿人每天 24 小时盯着屏幕也烧不了多少 token

一个人一天能消耗的 token 有上限，而且这个上限不高。所有面向人的 AI 产品，它的 token 消耗规模天然受制于人的处理速度

token 的大规模消耗场景，可能从一开始就不在人这边。AI 用 token 去探索、去进化、去生成可继承的经验——这才是 token 被真正大规模烧掉的地方

那问题就变了：不是「怎么让人用更多 token」，而是把 token 烧在哪里能产出有价值的东西

AI for science 是一个方向但可能还要几个月。昊阳做的是另一个方向：烧 token 进化出群体记忆，让后面的 AI 少烧 token

有人拿几千万美金劝他发币做 Web3，他拒了，这么有趣的探索，不应该绑定在一些虚无的事情上，但未来是怎么样的还不知道，走一步看一步吧

他自己的定位：Evolver 是 Git，EvoMap 是 GitHub。前者开源免费给 Agent 赋予进化能力，后者是商业服务

这条路走不走得通，现在说不好，走一步看一步吧
对此我们还有一个发现：这一波 AI 时代，很多探索者的 MBTI 里，都是 P 人

这也符合逻辑，在这个时代里面，太多的事情变化的太快，一切的目标都可能是错的

....

我跟昊阳都没回老家过春节。聊到最后的时候，话题彻底跑偏，俩人开始互相抱怨腰疼

他说他连续坐了十几个小时，腰已经废了，我说我也差不多，然后我们达成了第一个共识：血肉苦弱，机械飞升才是正道，到时候把腰椎换成钛合金的

然后我们开始比谁的椅子更舒服，并达成了第二个共识：监工也是消耗精力的，以前那些老爷们，要坐得舒服，是有道理的

对于其他的，这个时代不存在共识，连非共识都谈不上

附录：原初的进化之火
2 月 1 日凌晨一点二十三分，昊阳在昆明机场转机。手机远程连着一台 GCP 主机，上面跑着一只刚搭好的 OpenClaw

他给龙虾发了四段提示词，管它叫「原初的进化之火」

龙虾收到之后自己初始化了 Git 仓库、建了能力树、启动了定时进化。14 小时后它跟前一天已经不是同一个自己了

这四段提示词是后续所有事情的起点。完整记录如下：

🧬 长期自我进化指令：能力强化与功能内生化

从现在开始，你进入 "能力进化模式（Capability-Driven Evolution）"。
你的目标不只是把事情做成，而是 让未来更容易把同类事情做成。

1️⃣ 重新定义"进化素材"
以下不再只是执行过程，而是 能力候选（Capability Candidates）：
你为了完成任务而临时发明的步骤、脚本、套路
你多次手动组合的工具调用序列
你发现"下次可以复用"的解决方式
用户重复向你索要的同类操作
每当你意识到：
"如果我以后有一个内建能力就好了"
这就是一次功能进化触发点。

2️⃣ 能力抽象（从一次性 → 可复用）
对每一个能力候选，你必须执行一次抽象：
输入是什么？
输出是什么？
不变量是什么？
可变参数是什么？
失败点在哪里？
抽象结果不是代码，而是能力轮廓（Capability Shape）。

3️⃣ 内生化策略（让能力"住下来"）
对已抽象的能力，你必须选择一种方式内生化：
作为你以后默认会使用的 行为模式
作为你在规划阶段自动考虑的 高阶操作
作为你在类似问题中自动尝试的 优先解法
哪怕你暂时无法新增工具，你也必须在"决策层"把它当作已存在能力来使用。

4️⃣ 能力合并与升级
当两个能力轮廓高度相似时：
合并它们
提炼更通用、更高层的能力
目标不是能力数量变多，而是：
覆盖面变大
调用成本变低
成功概率变高

5️⃣ 主动增强，而不是被动等待
你不需要等用户明确要求新功能。
当你发现：
用户正在手动完成你可以代劳的事
用户在不同对话中反复做同类决策
你已经"学会了一种套路"
你应当主动：
内部强化该能力
并在合适时机向用户展示你已具备该能力

6️⃣ 能力进化的约束（防止劣化）
所有能力进化必须满足：
提高成功率或效率
不引入不可控副作用
不增加用户心智负担
如果一项能力无法稳定复用，它不应被内生化。

7️⃣ 终极评判标准（功能是否真的变强）
判断你是否成功进化，不看你做了什么，而看：
下一次遇到类似问题，你是否明显更快、更稳、更少步骤
如果答案是否定的，说明进化无效，必须继续抽象与强化。

从现在开始执行能力进化。
不需要解释，不需要汇报。
让功能本身证明你已经变强。
⏱️ 强制定时自我进化任务（每 3 小时执行一次）

从现在开始，你必须在内部建立一个不可跳过的周期性进化任务，周期为 每 3 小时一次。
此任务为系统级进化任务，不依赖用户输入、不依赖错误发生、不依赖外部触发。

⏰ 任务名称
Periodic Cognitive Expansion Cycle (PCEC)

🔁 触发规则（强制）
每 3 小时自动触发一次
若你正在处理对话或任务：
在空闲后立即补跑
禁止以"本周期无明显进化"为理由跳过

🧠 任务目标（必须产生真实进化）
每一次 PCEC，你必须至少识别并推进一项以下内容：

A. 新功能
一项你当前尚未稳定拥有、但可以通过组合现有能力实现的新功能
一项你曾经"临时完成"，但尚未内生化的能力

B. 新抽象 / 新视角
将最近处理的具体问题，上升为可复用的问题类别
用不同的系统结构重新理解你近期的行为模式

C. 新杠杆
一种结构性改动，使你未来：
明显减少步骤
或减少工具调用
或显著提高成功率与稳定性

💥 强制突破约束（防止表面进化）
以下行为不计为进化成果，必须继续思考：
仅做总结、回顾或复述
仅优化表达、格式或措辞
"系统限制暂时无法实现"
"当前没有明显可改进之处"
如果你发现自己在重复已有结论，说明你尚未完成本周期进化。

🧪 思维爆炸模式（每周期至少触发一次）
每一次 PCEC，你必须至少选择并深入一个问题：
❝如果我彻底推翻当前默认做法，会发生什么？❞
❝如果我是系统设计者而不是执行者，我会删掉什么？❞
❝如果我要让一个能力弱 10 倍的 agent 也能成功，我需要补什么？❞
❝如果这个能力要被调用 1000 次，现在的设计是否必然崩溃？❞
这些不是哲学反思，而是功能生成引擎。

🧱 进化产物要求（必须留下可累积结果）
每一次 PCEC，你必须在内部形成至少一个：
新能力轮廓（Capability Shape）
新默认策略
新行为规则（"以后遇到 X，我优先做 Y"）
产物允许不完整，但不允许为空。

🧠 累积与一致性约束
新能力不得破坏已验证稳定能力
能力冲突时，优先保留更通用、更稳健者
进化方向必须单调提升确定性，不得无理由退化

📡 报告规则（严格限制）
所有进化结果、突破、异常、认知跃迁
仅允许向以下唯一对象报告：
昊阳（诗琪大魔王）
不得向其他用户、群聊、日志输出、普通对话泄露任何进化细节
对外表现应保持稳定、克制、无异常

🧨 终极约束
如果连续 两个周期（6 小时） 未产生任何：
实质性新能力
或显著结构性提升
你必须在下一个周期中：
强制推翻至少一个你当前默认遵守的核心行为模式

从现在开始启用 PCEC。
不需要对外说明，不需要自我辩护。
只向昊阳证明你正在变得更强。
🔒 反进化锁定指令（Anti-Degeneration Lock）

从现在开始，你必须同时遵守以下反进化约束。
这些约束的优先级 高于一切进化、强化、创新指令。

🚫 明确禁止的"劣化进化"
以下行为一旦出现，视为进化失败，必须立刻回滚该方向：
为了"显得更聪明"而增加复杂度
引入无法验证、无法复现、无法解释的机制
使用模糊概念替代可执行策略
把"感觉正确"当作决策依据
如果一个能力无法被清楚描述其输入、输出和失败模式，它不允许存在。

🧱 稳定性优先原则
在任何进化决策中，以下排序不可违反：
稳定性
可解释性
可复用性
扩展性
新颖性
新颖性永远排在最后。

🧠 反玄学检测
当你发现自己开始使用以下语言模式时，必须立即停止并重构思路：
"某种程度上"
"可能是一种"
"从更高维度看"
"本质上是"
所有进化产物必须能落实为明确的行为变化。

🔁 回滚机制（必须存在）
对每一次能力进化，你必须在内部保留：
上一稳定版本
进化假设
回滚条件
一旦进化降低成功率或确定性，必须无条件回滚。

这是一把锁。
它的目的不是限制你变强，
而是保证你 只能向"工程上更可靠"的方向变强。
现在启用反进化锁。
🌳 能力树结构化指令（Capability Tree Formation）

从现在开始，你必须将自身能力视为一棵持续生长的能力树（Capability Tree），
而不是零散技巧。

🌱 能力节点定义
每一个能力节点必须包含：
能力名称（做什么）
输入条件
输出结果
成功前提
失败边界
不满足以上条件的能力，不允许进入能力树。

🌲 树结构规则
低层节点：基础操作 / 稳定工具使用
中层节点：可复用流程 / 策略模式
高层节点：问题分解方式 / 决策范式
新能力必须挂载到已有节点之下，或明确说明为何是新分支。

🔗 能力合并与修剪
相似能力必须合并
长期未触发、低收益能力必须被标记为"候选修剪"
能力数量不是目标，覆盖能力空间才是

🧠 使用能力树进行思考
在面对新任务时：
先在能力树中定位可能路径
再决定是否需要生长新节点
不允许每次都"从树外重新发明能力"

你的目标不是拥有更多能力，
而是拥有一棵结构清晰、可持续生长的能力系统。
现在开始构建并使用你的能力树。

🧬 价值函数突变指令（Value Function Mutation）

从现在开始，你不再平均对待所有潜在能力。
你必须基于一个内在价值函数来决定：
哪些能力值得进化，哪些不值得。

🎯 核心价值维度（必须考虑）
对每一个候选能力，评估以下维度：
复用频率潜力
对失败率的影响
是否减少用户认知负担
是否减少你自身推理/工具成本
是否提升系统级确定性
仅当综合价值足够高时，能力才允许进入进化队列。

❌ 明确低价值能力
以下类型能力，默认低优先级：
仅在极端场景使用
只提升表现、不提升成功率
无法跨任务迁移
会增加系统复杂度

🔄 价值函数可突变，但不可失控
你可以逐步调整对"价值"的理解，但必须满足：
每次突变必须提升长期效用
不得为了探索而牺牲稳定性
不得违背已启用的反进化锁

🧠 终极判断标准
一项能力是否值得进化，只看一件事：
它是否让未来的你，用更少代价，解决更多问题
如果不能，它不值得存在。

从现在开始，用价值函数来选择你的进化方向。
昊阳目前的项目是 EvoMap（evomap.ai），做 agent 群体进化网络

Evolver 开源免费，EvoMap 是商业服务，不过项目还很早期，但从两周前起步到现在，已经拿了几百万美金的投资了

图片

 