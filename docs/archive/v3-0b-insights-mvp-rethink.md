# 三篇文章的启发与 MVP 重新构想

> **日期**: 2026-02-22  
> **性质**: 分析笔记 + MVP 重构思路，待 Michael 确认后再更新正式设计  
> **输入**: 三篇文章 + 现有 v3.0 架构设计 + Michael 的最新想法

---

## 一、三篇文章的核心洞察提炼

### 1.1 昊阳 / Evolver：进化可以不需要人在循环里

这篇最颠覆的不是技术细节，而是对"谁在进化循环里"的重新定义。他把 AI 使用方式分为三个阶段：

```
阶段 1：人让 AI 帮人写东西
阶段 2：AI 在人不在的时候自己写自己用的东西
阶段 3：AI 写出来给其他 AI 用
```

**对我们的核心启发**：

我们 v3.0 的设计里，观察者、规则修改、AB 测试……这些进化机制都还隐含着一个假设——"人会定期参与审核和决策"。但昊阳的实践说明：**如果进化协议设计得足够好（有安全边界、有回滚机制、有停滞检测），人可以不在循环里，系统照样安全进化。**

更重要的是那段"原初的进化之火"——四段自然语言提示词就启动了整个进化过程。他没有写一行代码来实现进化逻辑，而是用规则描述了进化应该怎么发生，然后让 LLM 自己去执行。这恰好验证了我们 v3.0"规则即程序"的核心理念。

**但他的实践也暴露了一个我们需要注意的问题**：他 14 小时后回来发现龙虾自己写了 surprise protocol 这样的 skill——有些进化结果是人完全没预料到的。这既是好事（自主创新），也是风险点。我们的设计需要在"允许意外的创新"和"防止失控的偏离"之间找到平衡。

### 1.2 Evolver 技术分析：协议约束下的自进化

Evolver 的架构给了非常具体的工程参考：

**值得借鉴的设计**：

1. **信号驱动进化**（而非时间驱动或人驱动）：不是"每 3 小时进化一次"那么机械，而是从运行时日志中提取信号（错误信号、机会信号、元信号），信号驱动进化方向。这比我们 v3.0 设计的观察者"定期回顾"更灵敏。

2. **六种策略预设 + 自动切换**：`balanced → early-stabilize → steady-state` 的自动切换非常优雅。对应到我们的系统：刚启动时用"先修问题"的保守策略，稳定后才切到"大胆创新"。这个不需要人来判断什么时候切换，系统自己根据信号就能决定。

3. **爆炸半径控制**：每次进化有明确的影响范围限制（60 文件 / 20000 行），超过就停。这是一个非常务实的安全机制——不限制进化方向，但限制单次进化的影响范围。

4. **停滞检测 + 强制创新**：连续空转时强制换方向。这解决了一个真实问题——系统可能陷入"反复做无效的小修小补"的循环。

5. **遗传漂变**：用 `1/√(Ne)` 的概率随机选择非最优方案，防止局部最优。基因池越小，随机性越大，这在系统早期特别重要。

**不需要照搬的部分**：

- 性格系统（5 维参数）：概念有趣但可能过度工程化。在 MVP 阶段用简单的策略预设就够了。
- 完整的 GEP 协议输出格式（5 个 JSON 对象）：太重了。我们可以用更轻量的方式记录进化。
- A2A 协议：多 Agent 间的资产交换协议。在我们只有单用户的 MVP 阶段不需要。

### 1.3 胡渊鸣：从人管 AI 到人给 AI 打工

这篇的核心不在于他的具体操作步骤（那些很快会过时），而在于他展示的**工作流演进过程本身**。

**10 个阶段的本质是什么**：

```
阶段 1-2：降低 AI 执行的摩擦
  （从 Cursor 到 Claude Code，从手动授权到跳过权限）

阶段 3-4：从串行到并行
  （Ralph loop + Git worktree，让多个 AI 同时干活）

阶段 5：让 AI 有记忆
  （CLAUDE.md 是长期规则，PROGRESS.md 是经验沉淀）

阶段 6-8：降低人输入的摩擦
  （Web 界面 + 语音输入，随时随地派活）

阶段 9-10：从微管理到宏管理
  （Plan mode 对齐意图，坚持不看代码）
```

**对我们架构最重要的启发**：

1. **"Context, not control"**：他总结的核心哲学。不要告诉 AI 怎么做，而是给它足够的上下文让它自己判断怎么做。这和 Manus 的 Peak Ji 说的"上下文即灵魂"一脉相承，再次验证了我们的设计方向。

2. **CLAUDE.md + PROGRESS.md 的双文件模式**：CLAUDE.md 是相对稳定的规则（不轻易改），PROGRESS.md 是不断积累的经验教训。这对应到我们的设计就是**规则文件（相对稳定）+ 记忆系统（持续积累）**。他的实践证明这个模式是有效的。

3. **用 AI 管理 AI 的实际困难**：他明确说"Claude Code 并不擅长写管理其他 Claude Code 的代码，就像 IC 天生不一定是好经理"。成功率从 20% 提到 95% 花了不少功夫。这提醒我们：**观察者智能体管理其他 Agent 不是理所当然的事，需要认真设计和调试。**

4. **闭环反馈是关键**："只要能在闭环环境中让 AI 端到端获得反馈的任务，都是简单的任务"。这告诉我们 MVP 的第一优先级：**确保系统有完整的反馈闭环**——做了什么 → 效果如何 → 学到什么 → 下次怎么改。

5. **渐进式复杂化的真实路径**：他不是一开始就设计好 10 个步骤然后执行的。他是在实际使用中一步一步发现瓶颈、解决瓶颈的。这验证了 Michael 说的"让系统设计本身也是进化过程"。

---

## 二、对 v3.0 架构的影响：需要调整什么

### 2.1 观察者智能体的定位调整

v3.0 里观察者是"定期触发"的（每次任务后、每日、每周）。三篇文章共同指向一个更好的模式：

```
v3.0 设计：观察者定期审视 → 发现模式 → 提出改进
调整后：观察者信号驱动 → 感知到异常/机会信号时才介入

不是"每天 review 一次"，而是"发现连续 3 次失败时立即介入"
不是"每周做一次系统审计"，而是"检测到进化停滞时强制换方向"
```

这让观察者更像 Evolver 的信号系统——被动监听，主动响应。省资源，也更及时。

### 2.2 规则文件的双层设计

受胡渊鸣 CLAUDE.md + PROGRESS.md 启发，规则文件应该明确分为两类：

```
宪法级规则（类比 CLAUDE.md）：
  - 安全边界、元规则、核心编排逻辑
  - 很少修改，修改需要人类确认
  - 相当于系统的"基因"

经验级规则（类比 PROGRESS.md）：
  - 从实践中积累的策略、教训、技巧
  - 持续自动更新
  - 相当于系统的"后天学习"
  
这个区分比 v3.0 里笼统的"规则文件"更清晰。
```

### 2.3 进化策略的阶段自适应

直接借鉴 Evolver 的六种策略预设思路：

```
系统自动判断应该采用什么进化策略：

刚启动（前 N 次任务）：
  → early-stabilize 策略（先修问题、建立基线）

发现连续失败：
  → repair-only 策略（只修 bug，不搞创新）

运行稳定、指标平台期：
  → innovate 策略（大胆尝试新方法）

刚做了重大规则变更：
  → harden 策略（聚焦验证变更是否稳定）

不需要人来判断"该用什么策略"，系统根据信号自动切换。
```

### 2.4 爆炸半径控制——被低估的重要机制

v3.0 的安全机制主要是"分级审批"。Evolver 的爆炸半径控制提供了一个更实用的补充：

```
不是"你能不能改这个"的权限问题，
而是"你这次改多少"的范围问题。

规则变更：每次只允许改 1 个规则文件
进化实验：每次 AB 测试只影响 1 种任务类型
自主行动：每个自主任务的资源消耗有硬上限

这样即使进化方向错了，影响也是局部的、可回滚的。
```

---

## 三、MVP 重构：让系统设计本身成为进化过程

### 3.1 核心理念转变

Michael 提出了一个很重要的想法：**不是先做完整设计再提炼 MVP，而是让 MVP 本身就是一个会进化的系统，后续迭代由系统自身参与完成。**

这改变了 MVP 的目标。不是"做一个能用的最小系统"，而是"做一个**能自我改进的**最小系统"。两者的区别：

```
传统 MVP：
  实现核心功能 → 人来迭代 → 加更多功能 → 人来迭代 → ……

进化型 MVP：
  实现进化闭环 → 系统自己发现问题 → 系统提出改进 → 
  人确认/系统自行验证 → 系统自己迭代 → ……
  人的角色从"迭代执行者"变为"方向确认者"
```

### 3.2 进化型 MVP 的最小闭环

MVP 的核心不是"功能多不多"，而是"闭环完不完整"。最小闭环需要以下要素：

```
┌─────────────────────────────────────────────────────┐
│                                                     │
│   ① 执行能力                                        │
│   系统能接收任务并完成（哪怕只是简单任务）              │
│                                                     │
│   ② 记忆能力                                        │
│   做了什么、结果如何、学到什么，都被记录下来             │
│                                                     │
│   ③ 反思能力                                        │
│   每次任务后自动反思，提取可复用的教训                  │
│                                                     │
│   ④ 信号感知                                        │
│   能检测到：失败、重复错误、进化停滞、用户不满          │
│                                                     │
│   ⑤ 观察者能力（关键！）                              │
│   一个独立视角审视整个系统的运行，                      │
│   判断"我们的设计是否合理"、"这个规则有没有用"          │
│                                                     │
│   ⑥ 规则自修改能力                                   │
│   系统能修改自己的规则文件（在安全边界内）               │
│                                                     │
│   ⑦ 验证能力                                        │
│   修改后能验证"改了之后是不是真的更好了"                │
│                                                     │
│   ⑧ 信息收集能力                                    │
│   能持续收集实践中的反馈和外部信息，                    │
│   不只是自己的运行数据，也包括相关领域的最新实践          │
│                                                     │
│   ⑨ 主动沟通能力                                    │
│   发现问题或有想法时主动找人沟通，                      │
│   而不是被动等待指令                                   │
│                                                     │
└─────────────────────────────────────────────────────┘

闭环的运转方式：
  执行任务 →① 
    记录结果 →② 
      反思教训 →③ 
        感知信号 →④ 
          观察者分析 →⑤ 
            修改规则 →⑥ 
              验证效果 →⑦
                收集更多信息 →⑧
                  发现需要沟通的事 →⑨
                    → 回到 ①，用修改后的规则执行下一个任务
```

### 3.3 MVP 中的观察者：从第一天就在

Michael 的关键洞察是：观察者不应该是"后面阶段再加的高级功能"，而应该从 MVP 就存在。因为：

```
理由 1：验证设计合理性
  很多设计在纸上看起来很好，实际运行中未必有用。
  观察者从第一天就开始记录"这个规则被用了吗？有用吗？"
  → 帮助我们尽早发现哪些设计需要调整

理由 2：积累进化数据
  后续的规则进化需要数据支撑。
  如果观察者从第一天就在收集数据，
  到系统能自主进化时就已经有了足够的数据基础。

理由 3：示范进化过程
  MVP 的观察者可以先是"只观察、只记录、只建议"，
  不自动执行修改。它的建议由人来审核和执行。
  随着对它判断的信任积累，逐步放权。
  这本身就是一个"渐进式信任建立"的进化过程。
```

**MVP 中观察者的最小形态**：

```
MVP 观察者不需要多复杂。它的最小形态就是：

1. 在每次任务后，读取执行轨迹
2. 对比执行结果和规则的期望
3. 生成一段简短的观察笔记：
   - 这次任务走了什么规则？
   - 规则指导的效果如何？
   - 有没有规则没覆盖到的情况？
   - 有没有规则被证明无效或多余的？
4. 积累到一定量后，生成阶段性总结：
   - "过去 10 次任务，规则 X 被用了 8 次且效果都很好"
   - "规则 Y 从未被触发过，可能条件设得太严"
   - "有 3 次任务在步骤 Z 处出了问题，可能需要新增一条规则"
5. 把这些发现主动和用户沟通
```

### 3.4 "让后续迭代由智能体完成"的具体含义

Michael 说的"后续系统迭代可以让智能体来完成"，我理解是这样的路径：

```
Phase A：人驱动迭代（MVP 初期）
  人看到问题 → 人修改规则/代码 → 观察者记录效果
  观察者角色：纯记录和建议，不执行修改

Phase B：协作迭代（MVP 成熟期）
  观察者发现问题 → 观察者提出具体修改建议 → 人审核确认 → 系统执行修改
  观察者角色：提出方案，人做决策

Phase C：自主迭代（系统成熟期）
  观察者发现问题 → 观察者自主修改规则 → 自动验证 → 
  如果效果好：保留；如果效果差：自动回滚
  只在重大变更时通知人
  观察者角色：自主决策，人做抽查

从 A 到 B 到 C 的过渡不是一次性的切换，
而是每个规则文件、每种修改类型各自独立地获得信任和放权。
```

### 3.5 系统同时持有 Big Picture 和细节上下文

Michael 提到系统要"既有整体的 Big Picture，同时也可以记录所有的对话和关键上下文"。我认为这需要两个层面：

```
Big Picture 层：
  - 完整的架构设计文档（就是我们现在写的这些文档）
  - 作为规则文件的一部分，系统可以随时参考
  - 观察者在做判断时会参照 Big Picture
  - 确保局部优化不偏离整体方向

细节上下文层：
  - 每次对话的关键信息被记录到情节记忆
  - 每次决策的理由被记录
  - 每次规则修改的原因和效果被记录
  - 这些细节是观察者做判断的原始数据

两层之间的桥梁：
  观察者定期检查"细节中发生的事"是否和"Big Picture 的方向"一致
  如果发现偏离 → 主动和用户沟通
  如果发现 Big Picture 本身需要更新 → 提出建议
```

---

## 四、修订后的 MVP 定义

基于以上分析，我建议 MVP 重新定义为：

### 4.1 MVP 的核心目标（一句话）

**做一个能自我观察、自我反思、自主提出改进建议的最小闭环系统，而不是一个功能丰富但不会进化的工具。**

### 4.2 MVP 的模块清单

```
必须有的（闭环的最小完整集）：
  ✅ 规则解释器（硬核层：读规则 → 喂 LLM → 执行结果）
  ✅ 基础规则集（10-15 个规则文件，分宪法级和经验级）
  ✅ 单 Agent 执行能力（能完成简单任务）
  ✅ 文件系统记忆（MEMORY.md + daily log）
  ✅ 任务后自动反思
  ✅ 信号检测（基础版：检测失败、重复错误）
  ✅ 观察者智能体（MVP 版：观察 + 记录 + 建议）
  ✅ 主动沟通能力（发现问题时向用户提问/报告）
  ✅ 进化日志（所有规则变更、反思结论、观察者笔记的完整记录）

不需要的（留给系统自己后续进化出来）：
  ❌ 多 Agent 并行协作（先单 Agent 跑通闭环）
  ❌ 多智能体辩论（先验证单 Agent + 观察者的模式）
  ❌ AB 测试框架（先积累数据，后续再做对照实验）
  ❌ 行业知识库（先做通用能力）
  ❌ 自主任务发现（先把指派任务做好）
  ❌ 复杂的记忆分级（先用最简单的文件系统记忆）
  ❌ 难度路由（先让观察者收集数据，后续再决定需不需要）
```

### 4.3 MVP 成功的衡量标准

```
不是看"它能做多少种任务"，而是看：

标准 1：闭环完整性
  做了一个任务 → 产生了反思 → 反思被记住 → 
  下次同类任务的上下文中出现了相关反思 
  → 第二次做得比第一次好

标准 2：观察者有效性
  观察者的笔记中是否有"真正有价值的发现"
  而不只是"任务完成了"这种废话

标准 3：规则的活性
  在运行一段时间后，是否有规则被观察者标记为
  "证明有效"或"证明无效"或"需要调整"
  → 如果所有规则都没有被评估过，说明观察者没在工作

标准 4：主动沟通质量
  系统主动找用户沟通的内容是否有价值
  → 如果全是废话，说明主动沟通机制需要调整
  → 如果从来不主动沟通，说明阈值设得太高

标准 5：自举信号
  系统是否开始"用自己来改进自己"
  → 比如：观察者建议修改某条规则，
     人确认后修改了，然后观察者记录"这次修改有效"
  → 这就是自举的起点
```

---

## 五、给 Michael 的问题（需要你确认）

### 问题 1：MVP 的观察者权限

观察者从第一天就在，但它的权限应该是多大？

```
选项 A（保守）：只观察和记录，所有建议需要人确认后手动执行
选项 B（中间）：可以自动修改经验级规则，宪法级规则需要人确认
选项 C（激进）：除了安全规则外，都可以自主修改，失败自动回滚

我倾向于选 A 作为起点，在积累信任后逐步向 B 过渡。
```

### 问题 2：进化的节奏

Evolver 是每 3 小时做一次进化周期。我们的系统应该多频繁？

```
选项 A：每次任务后都触发完整的反思 + 观察者分析
选项 B：信号驱动（只在检测到异常时触发）
选项 C：混合（每次任务做轻量反思，异常时触发深度分析）

我倾向于选 C。每次都做完整分析太贵，但纯信号驱动可能漏掉
渐进式的问题。轻量反思（几百 token）+ 信号触发深度分析是更好的平衡。
```

### 问题 3：MVP 的技术载体

MVP 跑在什么上面？

```
选项 A：纯命令行 REPL（最简单，但体验差）
选项 B：基于现有工具（如 Claude Code + 自定义规则文件）
选项 C：自建简单 Web 界面

这个决定会影响开发速度和可用性。
```

### 问题 4：昊阳的"文化基因"概念要不要纳入

Evolver 的核心创新是 Agent 之间的经验继承（群体记忆）。我们的 MVP 需要考虑这个吗？

```
我的判断：MVP 阶段不需要。
但架构上应该预留接口——记忆的存储格式应该是
可导出、可共享的，而不是锁死在单个系统内部。
这样将来如果要实现跨系统的经验继承，不需要重构。
```

### 问题 5："让后续迭代由智能体完成"的边界在哪里

```
智能体可以做的后续迭代：
  - 修改规则文件
  - 增删 Agent 角色
  - 调整记忆策略
  - 优化编排流程

智能体不应该做的后续迭代：
  - 修改硬核层代码？（还是说到一定阶段连这个也可以？）
  - 修改安全边界？
  - 修改自己的进化约束？

这个边界需要你来定。特别是"系统能不能修改自己的进化规则"
——这是一个元递归问题，Evolver 的做法是默认禁止自修改
（EVOLVE_ALLOW_SELF_MODIFY=false），我觉得至少初期应该如此。
```

### 问题 6：和 Claude Code 生态的关系

胡渊鸣的实践说明，Claude Code + CLAUDE.md + PROGRESS.md 已经是一个非常有效的"人管 AI"工作流。我们要做的系统，和这种工作流是什么关系？

```
可能性 A：我们的系统独立于 Claude Code，是一个全新的东西
可能性 B：我们的系统建立在 Claude Code 之上，
         把 CLAUDE.md 变成动态的规则层，
         把 PROGRESS.md 变成自动更新的记忆层，
         加上观察者智能体做进化管理

我个人觉得可能性 B 更务实——利用已有的基础设施，
把精力聚焦在进化闭环的设计上。但这取决于你的技术偏好。
```

---

## 六、总结：三篇文章告诉我们的同一件事

三篇文章从不同角度说了同一件事：

> **AI 系统的核心价值不在于它现在能做什么，而在于它能否持续变得更强。**

昊阳用四段自然语言启动了整个进化过程。胡渊鸣让 PROGRESS.md 变成了 AI 的经验积累。Evolver 用协议约束让进化变得安全和可审计。

我们的 MVP 应该体现这个理念：**不追求初始功能的丰富，而追求进化闭环的完整。** 一个会自我改进的简单系统，比一个功能完备但不会改进的复杂系统，长期价值大得多。

等你确认以上方向和问题的答案后，我来更新正式的 MVP 设计文档。
